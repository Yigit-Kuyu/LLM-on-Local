## Info
This repository contains a Streamlit-based web application for document question-answering using large language models. The app allows users to upload PDF files, which are processed and stored in a vector database using Chroma and Ollama embeddings. Users can then ask questions about the uploaded documents, and the application uses a retrieval-augmented generation (RAG) approach to provide answers. The system leverages locally available Ollama models for text generation and supports multiple query generation for improved retrieval. Key features include PDF processing, persistent vector storage, multi-query retrieval, and a user-friendly chat interface. The application is designed to be easily deployable and customizable for various document QA scenarios.


![AV_Module](https://github.com/Yigit-Kuyu/LLM-on-Local/blob/main/Interface.JPG)
